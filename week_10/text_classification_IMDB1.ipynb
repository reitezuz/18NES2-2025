{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/reitezuz/18NES2-2025/blob/main/week_10/text_classification_IMDB1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VmbmIlvOehDz"
      },
      "source": [
        "# Simple Natural Language Processing Example - Sentiment Classification\n",
        "\n",
        "IMDB Dataset is a dataset for binary sentiment classification (positive or negative reviews). It contains a set of 25,000 highly polar movie reviews for training and 25,000 for testing.\n",
        "\n",
        " - http://ai.stanford.edu/~amaas/data/sentiment/\n",
        " - https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews\n",
        "\n",
        "Based on: https://github.com/fchollet/deep-learning-with-python-notebooks/blob/master/chapter14_text-classification.ipynb\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"KERAS_BACKEND\"] = \"jax\""
      ],
      "metadata": {
        "id": "5Pe3DTA6A3Uq"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6TOQxXr89Js3"
      },
      "source": [
        "\n",
        "\n",
        "### Download and extract the zip file with the data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -r aclImdb\n",
        "!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "!tar -xf aclImdb_v1.tar.gz\n",
        "!rm -r aclImdb/train/unsup\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4RS6gy86DoFz",
        "outputId": "0138f7a6-627e-401d-f0a8-43dc1d417f54"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm: cannot remove 'aclImdb': No such file or directory\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 80.2M  100 80.2M    0     0  57.1M      0  0:00:01  0:00:01 --:--:-- 57.1M\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, pathlib, shutil, random, keras\n",
        "\n",
        "imdb_extract_dir = pathlib.Path(\"aclImdb\")\n",
        "for path in imdb_extract_dir.glob(\"*/*\"):\n",
        "    if path.is_dir():\n",
        "        print(path)\n"
      ],
      "metadata": {
        "id": "8E2HvO4C62k3",
        "outputId": "79848ea1-f365-416b-f0ad-391854c241d8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "aclImdb/test/pos\n",
            "aclImdb/test/neg\n",
            "aclImdb/train/pos\n",
            "aclImdb/train/neg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# observe some reviews: the file name ends with the actual grade\n",
        "!cat aclImdb/train/pos/10014_8.txt\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WWX5ETqW2zW5",
        "outputId": "51accba3-6461-4152-c8e4-9542f2853528"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Night Listener held my attention, with Robin Williams shining as a New York City radio host who becomes enamored with his friendship with a 14 year old boy (Rory Culkin) who is very ill. Williams has never met the boy in person, as they have only been in contact by talking on the telephone. However, Williams' ex-boyfriend (nice job from Bobby Cannavale) raises doubt about the boy, which prompts Williams to arrange a meeting with him in person. What follows makes a permanent impact on Williams in a way he does not expect. I will leave it at that. Toni Collette also stars.<br /><br />I enjoyed this film, with Toni Collette giving a memorable portrayal of Culkin's adoptive mother. Sandra Oh also starred as Williams' friend. The Night Listener is inspired by actual events, and it has a somber, almost creepy silence throughout. At times it is predictable, no thanks to some of the reviews I read before seeing the movie and just due to logic, but I liked it anyway. I enjoy Williams in roles like this, more so than his comedic characters so that was an added bonus for me. Recommended. 8/10"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cat aclImdb/train/neg/10024_3.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bWobXyii4jSC",
        "outputId": "a9a72318-d410-4c63-9fe3-cf0165aa06e7"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First lesson that some film makers (particularly those inspired by Hollywood) need to know - just 'style' does not sell. I guess Tashan when translated will mean style. Second, if you are hell bent on selling style, that does not spare you from having a decent story.<br /><br />Tashan has some story which could have sufficed with some better director. But it is not slick. For example, all three - Saif, Kareena and Akshay - are narrators at different points in the story. But this setup is not utilized to properly. There could have been a better mix and match of their narrations. Actions sequences are from the seventies.<br /><br />Cheoreography of the film is awful. I think Vaibhavi Merchant just sleep walked through this film. Vishal-Shekhar have put up a good score but it does not belong to this film. Why is there a sufi song (Dil Haara) in Tashan? Why is the cool Hinglish song (Dil Dance Maare) not on Anil Kapoor when he is the one who is English crazy? <br /><br />Akshay Kumar is the saving grace of the film. But he is in his stereotyped self. You won't mind missing this film."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idtctNP5-VZA"
      },
      "source": [
        "## Prepare the data:\n",
        "1. divide the train folder into two folders: train and val\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "1-HcNN4XHvnj"
      },
      "outputs": [],
      "source": [
        "import os, pathlib, shutil, random\n",
        "\n",
        "# Create the directory with the validation data\n",
        "base_dir = pathlib.Path(\"aclImdb\")\n",
        "val_dir = base_dir / \"val\"\n",
        "train_dir = base_dir / \"train\"\n",
        "test_dir = base_dir / \"test\"\n",
        "\n",
        "\n",
        "os.makedirs(val_dir)\n",
        "for category in (\"neg\", \"pos\"):\n",
        "    os.makedirs(val_dir / category)\n",
        "    files = os.listdir(train_dir / category)\n",
        "    random.Random(1337).shuffle(files)\n",
        "    num_val_samples = int(0.2 * len(files))\n",
        "    val_files = files[-num_val_samples:]\n",
        "    for fname in val_files:\n",
        "        shutil.move(train_dir / category / fname,\n",
        "                    val_dir / category / fname)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for path in base_dir.glob(\"*/*\"):\n",
        "    if path.is_dir():\n",
        "        print(path)"
      ],
      "metadata": {
        "id": "HIblD3_N7z-l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac0b7409-7860-4525-d8f0-9e2ea44120b6"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "aclImdb/test/pos\n",
            "aclImdb/test/neg\n",
            "aclImdb/train/pos\n",
            "aclImdb/train/neg\n",
            "aclImdb/val/pos\n",
            "aclImdb/val/neg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "li6J_EmrXzdy"
      },
      "source": [
        "2. create the datasets from the directories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Q7TbmpFlTOEb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "291ae563-d11c-41ab-f3fd-2ee56dab4b1e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 20000 files belonging to 2 classes.\n",
            "Found 5000 files belonging to 2 classes.\n",
            "Found 25000 files belonging to 2 classes.\n"
          ]
        }
      ],
      "source": [
        "import keras\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "# Create the training, validation and testing data sets:\n",
        "train_ds = keras.utils.text_dataset_from_directory(\n",
        "    train_dir, batch_size=batch_size\n",
        ")\n",
        "val_ds = keras.utils.text_dataset_from_directory(\n",
        "    val_dir, batch_size=batch_size\n",
        ")\n",
        "test_ds = keras.utils.text_dataset_from_directory(\n",
        "    test_dir, batch_size=batch_size\n",
        ")\n",
        "text_only_train_ds = train_ds.map(lambda x, y: x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XcX_Y6kGHvnq"
      },
      "source": [
        "## The bag of words approach - 1. Unigrams\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dkibDdqbHvnr"
      },
      "source": [
        "### Preprocess the data\n",
        "- Configure the TextVectorization layer to return bag-of-words\n",
        "- Define the number of tokens to keep (skip the unimportant ones): choosing the 20,000 most important tokens is a reasonable choice.\n",
        "- Note: Vectorization always run on the CPU (we set num_parallel calls)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rAlKUEdjHvnr"
      },
      "outputs": [],
      "source": [
        "max_tokens = 20000\n",
        "max_length = 600      # Maximum length of each sequence (longer sequences will be truncated)\n",
        "\n",
        "\n",
        "text_vectorization = keras.layers.TextVectorization(\n",
        "    max_tokens=max_tokens,\n",
        "    output_mode=\"multi_hot\", # \"tf_idf\" \"counts\"\n",
        "    split = \"whitespace\"\n",
        ")\n",
        "\n",
        "# create vocabulary based on the daataset\n",
        "text_only_train_ds = train_ds.map(lambda x, y: x)\n",
        "text_vectorization.adapt(text_only_train_ds)\n",
        "\n",
        "bow_train_ds = train_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=8)\n",
        "bow_val_ds = val_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=8)\n",
        "bow_test_ds = test_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=8)\n",
        "\n",
        "for inputs, targets in bow_train_ds:\n",
        "    print(\"inputs.shape:\", inputs.shape)\n",
        "    print(\"inputs.dtype:\", inputs.dtype)\n",
        "    print(\"targets.shape:\", targets.shape)\n",
        "    print(\"targets.dtype:\", targets.dtype)\n",
        "    print(\"inputs[0]:\", inputs[0])\n",
        "    print(\"targets[0]:\", targets[0])\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 20 000 input features, batch size 32\n",
        "x, y = next(bow_train_ds.as_numpy_iterator())\n",
        "print(x.shape, y.shape)\n"
      ],
      "metadata": {
        "id": "nsGXZmkP_-ra"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# observe the vocabulary\n",
        "text_vectorization.get_vocabulary()[0:12]"
      ],
      "metadata": {
        "id": "L-LaND_tI7YV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_vectorization.get_vocabulary()[100:108]"
      ],
      "metadata": {
        "id": "xCQjCeEStpvp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define a MLP (or a simple linear classifier)"
      ],
      "metadata": {
        "id": "dGSct5kHwRQZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras import layers\n",
        "\n",
        "def get_mlp_model(max_tokens=20000, hidden_dim=None):\n",
        "    inputs = keras.Input(shape=(max_tokens,))\n",
        "    if hidden_dim is not None: # one hidden layer\n",
        "        x = layers.Dense(hidden_dim, activation=\"relu\")(inputs)\n",
        "        x = layers.Dropout(0.5)(x)\n",
        "    else: # simple linear classifier (no hidden layer)\n",
        "        x = inputs\n",
        "    outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "\n",
        "    model = keras.Model(inputs, outputs)\n",
        "    model.compile(optimizer=\"adam\",\n",
        "                  loss=\"binary_crossentropy\",\n",
        "                  metrics=[\"accuracy\"])\n",
        "    return model\n",
        "\n",
        "model = get_mlp_model()\n",
        "model.summary()\n"
      ],
      "metadata": {
        "id": "CkxwOyljvhKy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot the training progress:\n",
        "def plot_history(history):\n",
        "    history_dict = history.history\n",
        "    print(history_dict.keys())\n",
        "\n",
        "    from matplotlib import pyplot as plt\n",
        "\n",
        "    # Plot training & validation accuracy values\n",
        "    plt.plot(history.history['accuracy'])\n",
        "    plt.plot(history.history['val_accuracy'])\n",
        "    plt.title('Model accuracy')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "    plt.show()\n",
        "\n",
        "    # Plot training & validation loss values\n",
        "    plt.plot(history.history['loss'])\n",
        "    plt.plot(history.history['val_loss'])\n",
        "    plt.title('Model loss')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "2g7QLGXsFPQ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "results_df = pd.DataFrame()"
      ],
      "metadata": {
        "id": "Ws7DtQuosiNi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZRvCSwVGHvnr"
      },
      "source": [
        "### Train a MLP model on bag of words (unigrams)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6lMB_B4tHvns"
      },
      "outputs": [],
      "source": [
        "hidden_dim = None\n",
        "max_epochs = 10\n",
        "model_name = \"BOW_MLP.keras\"\n",
        "\n",
        "###############################################\n",
        "# Define the model architecture:\n",
        "model = get_mlp_model(max_tokens, hidden_dim)\n",
        "model.summary()\n",
        "\n",
        "################################################\n",
        "# Train the model\n",
        "import keras, time\n",
        "callbacks = [\n",
        "    keras.callbacks.EarlyStopping(monitor=\"val_loss\", restore_best_weights=True, patience=5, )\n",
        "]\n",
        "\n",
        "start_time = time.time()\n",
        "history = model.fit(bow_train_ds.cache(),\n",
        "          validation_data=bow_val_ds.cache(),\n",
        "          epochs=10,\n",
        "          callbacks=callbacks)\n",
        "time_fit = time.time() - start_time\n",
        "\n",
        "###############################\n",
        "# Plot the training progress:\n",
        "plot_history(history)\n",
        "\n",
        "train_loss, train_acc = model.evaluate(bow_train_ds)\n",
        "val_loss, val_acc = model.evaluate(bow_val_ds)\n",
        "test_loss, test_acc = model.evaluate(bow_test_ds)\n",
        "\n",
        "print(f\"Train acc: {train_acc:.3f}\")\n",
        "print(f\"Val acc: {val_acc:.3f}\")\n",
        "print(f\"Test acc: {test_acc:.3f}\")\n",
        "\n",
        "###############################\n",
        "# Save the model:\n",
        "import os\n",
        "model_dir = \"./models/\"\n",
        "if not os.path.exists(model_dir):\n",
        "    os.makedirs(model_dir)\n",
        "model.save(model_dir + model_name)\n",
        "\n",
        "#################################\n",
        "# Add results to the dataframe:\n",
        "new_entry = {\n",
        "    \"Model Name\" : model_name.strip(\".keras\"),\n",
        "    \"Details\" : str(max_tokens) + \" tokens, \" + str(hidden_dim),\n",
        "    \"Train Loss\" : train_loss,\n",
        "    \"Val Loss\" : val_loss,\n",
        "    \"Test Loss\" : test_loss,\n",
        "    \"Train Acc\" : train_acc,\n",
        "    \"Val Acc\" : val_acc,\n",
        "    \"Test Acc\" : test_acc,\n",
        "    \"Epochs\": max,\n",
        "    \"Time (s)\": time_fit\n",
        "}\n",
        "\n",
        "###############################\n",
        "if results_df.empty or \"results_df\" not in globals():\n",
        "    results_df = pd.DataFrame([new_entry])\n",
        "else:\n",
        "    results_df = pd.concat([results_df, pd.DataFrame([new_entry])], ignore_index=True)\n",
        "results_df.to_csv(\"results.csv\", index=False)\n",
        "results_df"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Apply the model to new texts"
      ],
      "metadata": {
        "id": "6VJ6geXZKV6p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import keras\n",
        "import tensorflow as tf\n",
        "\n",
        "base_model = keras.models.load_model(\"models/BOW_MLP.keras\")\n",
        "\n",
        "raw_text_data = tf.constant([\n",
        "    [\"That was an awful movie, I hate it.\"],\n",
        "    [\"Not worth seeing.\"],\n",
        "    [\"That was an excellent movie, I love it. Best movie ever.\"],\n",
        "    [\"I was shocked. Such an unexpected ending! Can't wait to see it again\"],\n",
        "    [\"I was shocked. The movie was too short. Can't wait to see it again\"],\n",
        "], dtype=tf.string)\n",
        "\n",
        "# Vectorize the raw text data using the adapted TextVectorization layer\n",
        "processed_raw_text_data = text_vectorization(raw_text_data)\n",
        "\n",
        "# Make predictions using the base model with the vectorized numerical input\n",
        "predictions = base_model(processed_raw_text_data)\n",
        "\n",
        "for i in range(len(raw_text_data)):\n",
        "    # Access the scalar value by indexing predictions[i] with [0]\n",
        "    print(f\"text {i}: {float(predictions[i][0] * 100):.2f} percent positive\")"
      ],
      "metadata": {
        "id": "VTDg1niMKVgz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JycdK52uHvnt"
      },
      "source": [
        "## The bag of words approach - 2. Bigrams\n",
        "\n",
        "### Preprocess the data\n",
        "- configure the TextVectorization layer to return binary encoded bigrams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EG8gkKFoHvnt"
      },
      "outputs": [],
      "source": [
        "max_tokens = 30000\n",
        "text_vectorization = layers.TextVectorization(\n",
        "    ngrams=2,                # change here\n",
        "    max_tokens=max_tokens,\n",
        "    output_mode= \"multi_hot\", # you can try also: \"count\", \"multi_hot\", \"tf_idf\"\n",
        ")\n",
        "\n",
        "text_vectorization.adapt(text_only_train_ds)\n",
        "bigram_train_ds = train_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=8)\n",
        "bigram_val_ds = val_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=8)\n",
        "bigram_test_ds = test_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=8)\n",
        "\n",
        "for inputs, targets in bigram_train_ds:\n",
        "    print(\"inputs.shape:\", inputs.shape)\n",
        "    print(\"inputs.dtype:\", inputs.dtype)\n",
        "    print(\"targets.shape:\", targets.shape)\n",
        "    print(\"targets.dtype:\", targets.dtype)\n",
        "    print(\"inputs[0]:\", inputs[0])\n",
        "    print(\"targets[0]:\", targets[0])\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x, y = next(bigram_train_ds.as_numpy_iterator())\n",
        "print(x.shape, y.shape)"
      ],
      "metadata": {
        "id": "BzED5xaVHrzE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_vectorization.get_vocabulary()[0:12]"
      ],
      "metadata": {
        "id": "9jt5aXDSBMUW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_vectorization.get_vocabulary()[100:108]"
      ],
      "metadata": {
        "id": "TKqeG8N-H9g6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cNJSDwXsHvnv"
      },
      "source": [
        "### Train a MLP model on binary encoded bigrams"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hidden_dim = None\n",
        "max_epochs = 10\n",
        "model_name = \"bigram_MLP.keras\"\n",
        "\n",
        "###############################################\n",
        "# Define the model architecture:\n",
        "model = get_mlp_model(max_tokens, hidden_dim)\n",
        "model.summary()\n",
        "\n",
        "################################################\n",
        "# Train the model\n",
        "import keras, time\n",
        "callbacks = [\n",
        "    keras.callbacks.EarlyStopping(monitor=\"val_loss\", restore_best_weights=True, patience=5, )\n",
        "]\n",
        "\n",
        "start_time = time.time()\n",
        "history = model.fit(bigram_train_ds.cache(),\n",
        "          validation_data=bigram_val_ds.cache(),\n",
        "          epochs=10,\n",
        "          callbacks=callbacks)\n",
        "time_fit = time.time() - start_time\n",
        "\n",
        "###############################\n",
        "# Plot the training progress:\n",
        "plot_history(history)\n",
        "\n",
        "train_loss, train_acc = model.evaluate(bigram_train_ds)\n",
        "val_loss, val_acc = model.evaluate(bigram_val_ds)\n",
        "test_loss, test_acc = model.evaluate(bigram_test_ds)\n",
        "\n",
        "print(f\"Train acc: {train_acc:.3f}\")\n",
        "print(f\"Val acc: {val_acc:.3f}\")\n",
        "print(f\"Test acc: {test_acc:.3f}\")\n",
        "\n",
        "###############################\n",
        "# Save the model:\n",
        "import os\n",
        "model_dir = \"./models/\"\n",
        "if not os.path.exists(model_dir):\n",
        "    os.makedirs(model_dir)\n",
        "model.save(model_dir + model_name)\n",
        "\n",
        "#################################\n",
        "# Add results to the dataframe:\n",
        "new_entry = {\n",
        "    \"Model Name\" : model_name.strip(\".keras\"),\n",
        "    \"Details\" : str(max_tokens) + \" tokens, \" + str(hidden_dim),\n",
        "    \"Train Loss\" : train_loss,\n",
        "    \"Val Loss\" : val_loss,\n",
        "    \"Test Loss\" : test_loss,\n",
        "    \"Train Acc\" : train_acc,\n",
        "    \"Val Acc\" : val_acc,\n",
        "    \"Test Acc\" : test_acc,\n",
        "    \"Epochs\": max,\n",
        "    \"Time (s)\": time_fit\n",
        "}\n",
        "\n",
        "###############################\n",
        "if results_df.empty or \"results_df\" not in globals():\n",
        "    results_df = pd.DataFrame([new_entry])\n",
        "else:\n",
        "    results_df = pd.concat([results_df, pd.DataFrame([new_entry])], ignore_index=True)\n",
        "# save the dataframe\n",
        "results_df.to_csv(\"results.csv\", index=False)\n",
        "results_df"
      ],
      "metadata": {
        "id": "EZNc6hB-Jdf8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# remove the second row from df:\n",
        "#results_df = results_df.drop(1)\n",
        "#results_df"
      ],
      "metadata": {
        "id": "IZVUqBH5Lki_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# remve big datasets from memory\n",
        "\n",
        "del bigram_train_ds\n",
        "del bigram_val_ds\n",
        "del bigram_test_ds\n"
      ],
      "metadata": {
        "id": "VZ3z7P3YENiV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4kfeno1OHvnw"
      },
      "source": [
        "### Apply the model to new texts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9jyXn0jhHvnw"
      },
      "outputs": [],
      "source": [
        "import keras\n",
        "import tensorflow as tf\n",
        "\n",
        "base_model = keras.models.load_model(\"models/bigram_MLP.keras\")\n",
        "\n",
        "raw_text_data = tf.constant([\n",
        "    [\"That was an awful movie, I hate it.\"],\n",
        "    [\"Not worth seeing.\"],\n",
        "    [\"That was an excellent movie, I love it. Best movie ever.\"],\n",
        "    [\"I was shocked. Such an unexpected ending! Can't wait to see it again\"],\n",
        "    [\"I was shocked. The movie was too short. Can't wait to see it again\"],\n",
        "], dtype=tf.string)\n",
        "\n",
        "# Vectorize the raw text data using the adapted TextVectorization layer\n",
        "processed_raw_text_data = text_vectorization(raw_text_data)\n",
        "\n",
        "# Make predictions using the base model with the vectorized numerical input\n",
        "predictions = base_model(processed_raw_text_data)\n",
        "\n",
        "for i in range(len(raw_text_data)):\n",
        "    # Access the scalar value by indexing predictions[i] with [0]\n",
        "    print(f\"text {i}: {float(predictions[i][0] * 100):.2f} percent positive\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "__Observation:__ better results than for unigrams"
      ],
      "metadata": {
        "id": "OcQv4do1LUUS"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9iduNxqKS3Ai"
      },
      "source": [
        "## Sequential model on one-hot vectors\n",
        "- all sequences are truncated or padded to the length 600\n",
        "- we use the biridectional LSTM model\n",
        "- the training is very slow and unefficient"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from keras import layers\n",
        "\n",
        "max_length = 600      # Maximum length of each sequence (longer sequences will be truncated)\n",
        "max_tokens = 20000    # Number of (most important) tokens\n",
        "\n",
        "text_vectorization = layers.TextVectorization(\n",
        "    max_tokens=max_tokens,\n",
        "    output_mode=\"int\",                 # Convert text to sequences of integer indices\n",
        "    output_sequence_length=max_length, # Ensure sequences have the given fixed length\n",
        ")\n",
        "\n",
        "# This step analyzes the dataset to create a vocabulary based on the most frequent tokens\n",
        "text_vectorization.adapt(text_only_train_ds)\n",
        "\n",
        "int_train_ds = train_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "int_val_ds = val_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "int_test_ds = test_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "\n",
        "# Observe the data:\n",
        "for inputs, targets in int_train_ds:\n",
        "      print(\"inputs.shape:\", inputs.shape)\n",
        "      print(\"inputs.dtype:\", inputs.dtype)\n",
        "      print(\"targets.shape:\", targets.shape)\n",
        "      print(\"targets.dtype:\", targets.dtype)\n",
        "      print(\"inputs:\", inputs)\n",
        "      print(\"targets:\", targets)\n",
        "      break\n",
        "\n"
      ],
      "metadata": {
        "id": "VilldIi0Q-k4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x, y = next(int_test_ds.as_numpy_iterator())\n",
        "x.shape, y.shape"
      ],
      "metadata": {
        "id": "N0s0Hc9RaTqo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras import ops # Ensure ops is imported\n",
        "\n",
        "rnn_units = 32\n",
        "max_epochs = 10\n",
        "model_name = \"one_hot_bidir_lstm.keras\"\n",
        "\n",
        "###############################################\n",
        "# one-hot encode the input sequence:\n",
        "inputs = keras.Input(shape=(max_length,), dtype=\"int64\") # Input shape is (None, max_length)\n",
        "one_hot_encoded = layers.Lambda(lambda x: ops.one_hot(x, max_tokens), output_shape=(max_length, max_tokens))(inputs)\n",
        "embedded = layers.Lambda(lambda x: ops.cast(x, \"float32\"))(one_hot_encoded)\n",
        "\n",
        "# This 3D tensor is compatible with Bidirectional(LSTM).\n",
        "x = layers.Bidirectional(layers.LSTM(rnn_units))(embedded)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "\n",
        "model.compile(optimizer=\"adam\",\n",
        "              loss=\"binary_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "model.summary()\n",
        "\n",
        "################################################\n",
        "# Train the model\n",
        "import keras, time\n",
        "callbacks = [\n",
        "    keras.callbacks.EarlyStopping(monitor=\"val_loss\", restore_best_weights=True, patience=5, )\n",
        "]\n",
        "\n",
        "start_time = time.time()\n",
        "history = model.fit(int_train_ds.cache(),\n",
        "          validation_data=int_val_ds.cache(),\n",
        "          epochs=10,\n",
        "          callbacks=callbacks)\n",
        "time_fit = time.time() - start_time\n",
        "\n",
        "###############################\n",
        "# Plot the training progress:\n",
        "plot_history(history)\n",
        "\n",
        "train_loss, train_acc = model.evaluate(int_train_ds)\n",
        "val_loss, val_acc = model.evaluate(int_val_ds)\n",
        "test_loss, test_acc = model.evaluate(int_test_ds)\n",
        "\n",
        "print(f\"Train acc: {train_acc:.3f}\")\n",
        "print(f\"Val acc: {val_acc:.3f}\")\n",
        "print(f\"Test acc: {test_acc:.3f}\")\n",
        "\n",
        "###############################\n",
        "# Save the model:\n",
        "import os\n",
        "model_dir = \"./models/\"\n",
        "if not os.path.exists(model_dir):\n",
        "    os.makedirs(model_dir)\n",
        "model.save(model_dir + model_name)\n",
        "\n",
        "#################################\n",
        "# Add results to the dataframe:\n",
        "new_entry = {\n",
        "    \"Model Name\" : model_name.strip(\".keras\"),\n",
        "    \"Details\" : str(max_tokens) + \" tokens, \" + str(hidden_dim),\n",
        "    \"Train Loss\" : train_loss,\n",
        "    \"Val Loss\" : val_loss,\n",
        "    \"Test Loss\" : test_loss,\n",
        "    \"Train Acc\" : train_acc,\n",
        "    \"Val Acc\" : val_acc,\n",
        "    \"Test Acc\" : test_acc,\n",
        "    \"Epochs\": max,\n",
        "    \"Time (s)\": time_fit\n",
        "}\n",
        "\n",
        "###############################\n",
        "if results_df.empty or \"results_df\" not in globals():\n",
        "    results_df = pd.DataFrame([new_entry])\n",
        "else:\n",
        "    results_df = pd.concat([results_df, pd.DataFrame([new_entry])], ignore_index=True)\n",
        "# save the dataframe\n",
        "results_df.to_csv(\"results.csv\", index=False)\n",
        "results_df"
      ],
      "metadata": {
        "id": "xtloaN1nVmcy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Apply the model to new data"
      ],
      "metadata": {
        "id": "4YnTIBk1JTG6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import keras\n",
        "import tensorflow as tf\n",
        "\n",
        "base_model = model\n",
        "\n",
        "raw_text_data = tf.constant([\n",
        "    [\"That was an awful movie, I hate it.\"],\n",
        "    [\"Not worth seeing.\"],\n",
        "    [\"That was an excellent movie, I love it. Best movie ever.\"],\n",
        "    [\"I was shocked. Such an unexpected ending! Can't wait to see it again\"],\n",
        "    [\"I was shocked. The movie was too short. Can't wait to see it again\"],\n",
        "], dtype=tf.string)\n",
        "\n",
        "# Vectorize the raw text data using the adapted TextVectorization layer\n",
        "processed_raw_text_data = text_vectorization(raw_text_data)\n",
        "\n",
        "# Make predictions using the base model with the vectorized numerical input\n",
        "predictions = base_model(processed_raw_text_data)\n",
        "\n",
        "for i in range(len(raw_text_data)):\n",
        "    # Access the scalar value by indexing predictions[i] with [0]\n",
        "    print(f\"text {i}: {float(predictions[i][0] * 100):.2f} percent positive\")"
      ],
      "metadata": {
        "id": "-GX3vqTCJNXW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OUJOq-EbbIIN"
      },
      "source": [
        "## Sequential model that uses an Embedding layer trained from scratch\n",
        "- __Embedding__ layer: https://keras.io/api/layers/core_layers/embedding/\n",
        "\n",
        "`layers.Embedding(input_dim=max_tokens, output_dim=embedding_dim, mask_zero=True)`\n",
        "\n",
        "- we set `mask_zero=True` to let the model ignore padding positions during training  \n",
        "  (padding tokens with index 0 are not included in the computation of LSTM/GRU states)\n",
        "  - this usually improves performance on variable-length text sequences"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rnn_units = 32\n",
        "embedding_dim = 64 #256\n",
        "max_epochs = 10\n",
        "model_name = \"embeddings_bidir_gru.keras\"\n",
        "\n",
        "###############################################\n",
        "# Define the model architecture:\n",
        "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "\n",
        "# Add an Embedding layer to convert integer tokens into dense vectors\n",
        "embedded = layers.Embedding(\n",
        "      input_dim=max_tokens,\n",
        "      output_dim=embedding_dim,\n",
        "      mask_zero=True\n",
        "    )(inputs)\n",
        "x = layers.Bidirectional(layers.GRU(rnn_units))(embedded)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.compile(optimizer=\"adam\",\n",
        "              loss=\"binary_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "model.summary()\n",
        "\n",
        "################################################\n",
        "# Train the model\n",
        "import keras, time\n",
        "callbacks = [\n",
        "    keras.callbacks.EarlyStopping(monitor=\"val_loss\", restore_best_weights=True, patience=5, )\n",
        "]\n",
        "\n",
        "start_time = time.time()\n",
        "history = model.fit(int_train_ds.cache(),\n",
        "          validation_data=int_val_ds.cache(),\n",
        "          epochs=max_epochs,\n",
        "          callbacks=callbacks)\n",
        "time_fit = time.time() - start_time\n",
        "\n",
        "###############################\n",
        "# Plot the training progress:\n",
        "plot_history(history)\n",
        "\n",
        "train_loss, train_acc = model.evaluate(int_train_ds)\n",
        "val_loss, val_acc = model.evaluate(int_val_ds)\n",
        "test_loss, test_acc = model.evaluate(int_test_ds)\n",
        "\n",
        "print(f\"Train acc: {train_acc:.3f}\")\n",
        "print(f\"Val acc: {val_acc:.3f}\")\n",
        "print(f\"Test acc: {test_acc:.3f}\")\n",
        "\n",
        "###############################\n",
        "# Save the model:\n",
        "import os\n",
        "model_dir = \"./models/\"\n",
        "if not os.path.exists(model_dir):\n",
        "    os.makedirs(model_dir)\n",
        "model.save(model_dir + model_name)\n",
        "\n",
        "#################################\n",
        "# Add results to the dataframe:\n",
        "new_entry = {\n",
        "    \"Model Name\" : model_name.strip(\".keras\"),\n",
        "    \"Details\" : str(max_tokens) + \" tokens, \" + str(hidden_dim),\n",
        "    \"Train Loss\" : train_loss,\n",
        "    \"Val Loss\" : val_loss,\n",
        "    \"Test Loss\" : test_loss,\n",
        "    \"Train Acc\" : train_acc,\n",
        "    \"Val Acc\" : val_acc,\n",
        "    \"Test Acc\" : test_acc,\n",
        "    \"Epochs\": max,\n",
        "    \"Time (s)\": time_fit\n",
        "}\n",
        "\n",
        "###############################\n",
        "if results_df.empty or \"results_df\" not in globals():\n",
        "    results_df = pd.DataFrame([new_entry])\n",
        "else:\n",
        "    results_df = pd.concat([results_df, pd.DataFrame([new_entry])], ignore_index=True)\n",
        "# save the dataframe\n",
        "results_df.to_csv(\"results.csv\", index=False)\n",
        "results_df"
      ],
      "metadata": {
        "id": "jLNyFO4GZE_D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "__Observation:__\n",
        "- the training is slow (in our case about 40 minutes on google colab CPU)\n",
        "- the model overfitts early\n",
        "- the results are worse than for the bigram+MLP approach: most probably because of the limit on sequence length (600 words)"
      ],
      "metadata": {
        "id": "Yhc0Aqp8fJ52"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Apply the model to new data"
      ],
      "metadata": {
        "id": "AG9-FEx7tRSj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create an inference model:\n",
        "import keras\n",
        "inputs = keras.Input(shape=(1,), dtype=\"string\")\n",
        "model = keras.models.load_model(\"models/embeddings_bidir_gru.keras\")\n",
        "processed_inputs = text_vectorization(inputs)\n",
        "outputs = model(processed_inputs)\n",
        "inference_model = keras.Model(inputs, outputs)\n",
        "\n",
        "import tensorflow as tf\n",
        "raw_text_data = tf.constant([\n",
        "    [\"That was an awful movie, I hate it.\"],\n",
        "    [\"Not worth seeing.\"],\n",
        "    [\"That was an excellent movie, I love it. Best movie ever.\"],\n",
        "    [\"I was shocked. The movie was too short. Can't wait to see it again\"],\n",
        "    [\"I was shocked. Such an unexpected ending! Can't wait to see it again\"],\n",
        "], dtype=tf.string)\n",
        "predictions = inference_model(raw_text_data)\n",
        "for i in range(len(raw_text_data)):\n",
        "    print(f\"text {i}: {float(predictions[i] * 100):.2f} percent positive\")"
      ],
      "metadata": {
        "id": "WYWBHKIis_Gh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lyQLJpqcgpBr"
      },
      "source": [
        "## Sequential model that uses existing pretrained word embeddings\n",
        "- useful for small datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9r_JjWbKgwOd"
      },
      "outputs": [],
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip -q glove.6B.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rVWhnSmSg2ZN"
      },
      "outputs": [],
      "source": [
        "# Parse the GloVe word-embeddings file\n",
        "import numpy as np\n",
        "path_to_glove_file = \"glove.6B.100d.txt\"\n",
        "\n",
        "embeddings_index = {}\n",
        "with open(path_to_glove_file) as f:\n",
        "    for line in f:\n",
        "        word, coefs = line.split(maxsplit=1)\n",
        "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
        "        embeddings_index[word] = coefs\n",
        "\n",
        "print(f\"Found {len(embeddings_index)} word vectors.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hTuk1fZFg8r8"
      },
      "outputs": [],
      "source": [
        "# Prepare the GloVe word-embeddings matrix\n",
        "embedding_dim = 100\n",
        "\n",
        "vocabulary = text_vectorization.get_vocabulary()\n",
        "word_index = dict(zip(vocabulary, range(len(vocabulary))))\n",
        "\n",
        "embedding_matrix = np.zeros((max_tokens, embedding_dim))\n",
        "for word, i in word_index.items():\n",
        "    if i < max_tokens:\n",
        "        embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iJeEzBt-hFVn"
      },
      "outputs": [],
      "source": [
        "# Embedding layer\n",
        "embedding_layer = layers.Embedding(\n",
        "    max_tokens,\n",
        "    embedding_dim,\n",
        "    embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
        "    trainable=False,\n",
        "    mask_zero=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hidden_dim = 32\n",
        "embedding_dim = 64 #256\n",
        "max_epochs = 10\n",
        "model_name = \"embeddings_glove_gru.keras\"\n",
        "\n",
        "###############################################\n",
        "# Define the model architecture:\n",
        "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "\n",
        "# Add an Embedding layer to convert integer tokens into dense vectors\n",
        "embedded = embedding_layer(inputs)\n",
        "x = layers.Bidirectional(layers.GRU(hidden_dim))(embedded)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.compile(optimizer=\"adam\",\n",
        "              loss=\"binary_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "model.summary()\n",
        "\n",
        "################################################\n",
        "# Train the model\n",
        "import keras, time\n",
        "callbacks = [\n",
        "    keras.callbacks.EarlyStopping(monitor=\"val_loss\", restore_best_weights=True, patience=5, )\n",
        "]\n",
        "\n",
        "start_time = time.time()\n",
        "history = model.fit(int_train_ds.cache(),\n",
        "          validation_data=int_val_ds.cache(),\n",
        "          epochs=max_epochs,\n",
        "          callbacks=callbacks)\n",
        "time_fit = time.time() - start_time\n",
        "\n",
        "###############################\n",
        "# Plot the training progress:\n",
        "plot_history(history)\n",
        "\n",
        "train_loss, train_acc = model.evaluate(int_train_ds)\n",
        "val_loss, val_acc = model.evaluate(int_val_ds)\n",
        "test_loss, test_acc = model.evaluate(int_test_ds)\n",
        "\n",
        "print(f\"Train acc: {train_acc:.3f}\")\n",
        "print(f\"Val acc: {val_acc:.3f}\")\n",
        "print(f\"Test acc: {test_acc:.3f}\")\n",
        "\n",
        "###############################\n",
        "# Save the model:\n",
        "import os\n",
        "model_dir = \"./models/\"\n",
        "if not os.path.exists(model_dir):\n",
        "    os.makedirs(model_dir)\n",
        "model.save(model_dir + model_name)\n",
        "\n",
        "#################################\n",
        "# Add results to the dataframe:\n",
        "new_entry = {\n",
        "    \"Model Name\" : model_name.strip(\".keras\"),\n",
        "    \"Details\" : str(max_tokens) + \" tokens, \" + str(hidden_dim),\n",
        "    \"Train Loss\" : train_loss,\n",
        "    \"Val Loss\" : val_loss,\n",
        "    \"Test Loss\" : test_loss,\n",
        "    \"Train Acc\" : train_acc,\n",
        "    \"Val Acc\" : val_acc,\n",
        "    \"Test Acc\" : test_acc,\n",
        "    \"Epochs\": max,\n",
        "    \"Time (s)\": time_fit\n",
        "}\n",
        "\n",
        "###############################\n",
        "if results_df.empty or \"results_df\" not in globals():\n",
        "    results_df = pd.DataFrame([new_entry])\n",
        "else:\n",
        "    results_df = pd.concat([results_df, pd.DataFrame([new_entry])], ignore_index=True)\n",
        "# save the dataframe\n",
        "results_df.to_csv(\"results.csv\", index=False)\n",
        "results_df"
      ],
      "metadata": {
        "id": "sMB-ML_Tslu_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Apply the model to new data"
      ],
      "metadata": {
        "id": "U4YZLK4U4ds7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import keras\n",
        "import tensorflow as tf\n",
        "\n",
        "base_model = keras.models.load_model(\"models/embeddings_glove_gru.keras\")\n",
        "\n",
        "raw_text_data = tf.constant([\n",
        "    [\"That was an awful movie, I hate it.\"],\n",
        "    [\"Not worth seeing.\"],\n",
        "    [\"That was an excellent movie, I love it. Best movie ever.\"],\n",
        "    [\"I was shocked. Such an unexpected ending! Can't wait to see it again\"],\n",
        "    [\"I was shocked. The movie was too short. Can't wait to see it again\"],\n",
        "], dtype=tf.string)\n",
        "\n",
        "# Vectorize the raw text data using the adapted TextVectorization layer\n",
        "processed_raw_text_data = text_vectorization(raw_text_data)\n",
        "\n",
        "# Make predictions using the base model with the vectorized numerical input\n",
        "predictions = base_model(processed_raw_text_data)\n",
        "\n",
        "for i in range(len(raw_text_data)):\n",
        "    # Access the scalar value by indexing predictions[i] with [0]\n",
        "    print(f\"text {i}: {float(predictions[i][0] * 100):.2f} percent positive\")"
      ],
      "metadata": {
        "id": "Zt-eco93tf-K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "__Observations:__ worse results than training embedding from scratch for domain-specific sentences (e.g., last two)"
      ],
      "metadata": {
        "id": "--VqHDH95rwV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sequential model that uses uses word embeddings pretrained by a CBOW model trained from scratch"
      ],
      "metadata": {
        "id": "X--3WMoVhlwJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "imdb_vocabulary = text_vectorization.get_vocabulary()\n",
        "tokenize_no_padding = keras.layers.TextVectorization(\n",
        "    vocabulary=imdb_vocabulary,\n",
        "    split=\"whitespace\",\n",
        "    output_mode=\"int\",\n",
        ")"
      ],
      "metadata": {
        "id": "my1V9nxigxGy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Preprocess the data for the CBOW model\n",
        "- create windows of tokens from the training data, where each window consists of context words and a target word.\n",
        "- The `window_data` function generates sliding windows of tokens, and `split_label`\n",
        " separates the context (bag) from the target (label) word within each window.\n",
        "- The dataset is then mapped through a tokenizer, windowed, and split into (context, target) pairs."
      ],
      "metadata": {
        "id": "Glsv6BoiI57n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "context_size = 4\n",
        "window_size = 9\n",
        "\n",
        "def window_data(token_ids):\n",
        "    num_windows = tf.maximum(tf.size(token_ids) - context_size * 2, 0)\n",
        "    windows = tf.range(window_size)[None, :]\n",
        "    windows = windows + tf.range(num_windows)[:, None]\n",
        "    windowed_tokens = tf.gather(token_ids, windows)\n",
        "    return tf.data.Dataset.from_tensor_slices(windowed_tokens)\n",
        "\n",
        "def split_label(window):\n",
        "    left = window[:context_size]\n",
        "    right = window[context_size + 1 :]\n",
        "    bag = tf.concat((left, right), axis=0)\n",
        "    label = window[4]\n",
        "    return bag, label\n",
        "\n",
        "dataset = keras.utils.text_dataset_from_directory(\n",
        "    imdb_extract_dir / \"train\", batch_size=None\n",
        ")\n",
        "dataset = dataset.map(lambda x, y: x, num_parallel_calls=8)\n",
        "dataset = dataset.map(tokenize_no_padding, num_parallel_calls=8)\n",
        "dataset = dataset.interleave(window_data, cycle_length=8, num_parallel_calls=8)\n",
        "dataset = dataset.map(split_label, num_parallel_calls=8)\n",
        "\n"
      ],
      "metadata": {
        "id": "oM6obDFug085"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train the CBOW  model"
      ],
      "metadata": {
        "id": "0oy0WgHuIdeu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hidden_dim = 64\n",
        "inputs = keras.Input(shape=(2 * context_size,))\n",
        "cbow_embedding = layers.Embedding(\n",
        "    max_tokens,\n",
        "    hidden_dim,\n",
        ")\n",
        "x = cbow_embedding(inputs)\n",
        "x = layers.GlobalAveragePooling1D()(x)\n",
        "outputs = layers.Dense(max_tokens, activation=\"sigmoid\")(x)\n",
        "cbow_model = keras.Model(inputs, outputs)\n",
        "cbow_model.compile(\n",
        "    optimizer=\"adam\",\n",
        "    loss=\"sparse_categorical_crossentropy\",\n",
        "    metrics=[\"sparse_categorical_accuracy\"],\n",
        ")\n",
        "\n",
        "cbow_model.summary(line_length=80)\n",
        "\n",
        "dataset = dataset.batch(1024).cache()\n",
        "cbow_model.fit(dataset, epochs=4)\n",
        "\n"
      ],
      "metadata": {
        "id": "WbAmtZ-vg8dD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Using the pretrained embedding for classification"
      ],
      "metadata": {
        "id": "-zYSVk5OhfoG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gru_dim = 32\n",
        "embedding_dim = 64 #256\n",
        "max_epochs = 10\n",
        "model_name = \"embeddings_cbow_gru.keras\"\n",
        "\n",
        "###############################################\n",
        "# Define the model architecture:\n",
        "inputs = keras.Input(shape=(max_length,))\n",
        "lstm_embedding = layers.Embedding(\n",
        "    input_dim=max_tokens,\n",
        "    output_dim=embedding_dim,\n",
        "    mask_zero=True,\n",
        ")\n",
        "x = lstm_embedding(inputs)\n",
        "x = layers.Bidirectional(layers.GRU(gru_dim))(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "\n",
        "\n",
        "model = keras.Model(inputs, outputs, name=\"gru_with_cbow\")\n",
        "\n",
        "lstm_embedding.embeddings.assign(cbow_embedding.embeddings)\n",
        "\n",
        "model.compile(\n",
        "    optimizer=\"adam\",\n",
        "    loss=\"binary_crossentropy\",\n",
        "    metrics=[\"accuracy\"],\n",
        ")\n",
        "\n",
        "################################################\n",
        "# Train the model\n",
        "import keras, time\n",
        "callbacks = [\n",
        "    keras.callbacks.EarlyStopping(monitor=\"val_loss\", restore_best_weights=True, patience=5, )\n",
        "]\n",
        "\n",
        "start_time = time.time()\n",
        "history = model.fit(int_train_ds.cache(),\n",
        "          validation_data=int_val_ds.cache(),\n",
        "          epochs=max_epochs,\n",
        "          callbacks=callbacks)\n",
        "time_fit = time.time() - start_time\n",
        "\n",
        "###############################\n",
        "# Plot the training progress:\n",
        "plot_history(history)\n",
        "\n",
        "train_loss, train_acc = model.evaluate(int_train_ds)\n",
        "val_loss, val_acc = model.evaluate(int_val_ds)\n",
        "test_loss, test_acc = model.evaluate(int_test_ds)\n",
        "\n",
        "print(f\"Train acc: {train_acc:.3f}\")\n",
        "print(f\"Val acc: {val_acc:.3f}\")\n",
        "print(f\"Test acc: {test_acc:.3f}\")\n",
        "\n",
        "###############################\n",
        "# Save the model:\n",
        "import os\n",
        "model_dir = \"./models/\"\n",
        "if not os.path.exists(model_dir):\n",
        "    os.makedirs(model_dir)\n",
        "model.save(model_dir + model_name)\n",
        "\n",
        "#################################\n",
        "# Add results to the dataframe:\n",
        "new_entry = {\n",
        "    \"Model Name\" : model_name.strip(\".keras\"),\n",
        "    \"Details\" : str(max_tokens) + \" tokens, \" + str(hidden_dim),\n",
        "    \"Train Loss\" : train_loss,\n",
        "    \"Val Loss\" : val_loss,\n",
        "    \"Test Loss\" : test_loss,\n",
        "    \"Train Acc\" : train_acc,\n",
        "    \"Val Acc\" : val_acc,\n",
        "    \"Test Acc\" : test_acc,\n",
        "    \"Epochs\": max,\n",
        "    \"Time (s)\": time_fit\n",
        "}\n",
        "\n",
        "###############################\n",
        "if results_df.empty or \"results_df\" not in globals():\n",
        "    results_df = pd.DataFrame([new_entry])\n",
        "else:\n",
        "    results_df = pd.concat([results_df, pd.DataFrame([new_entry])], ignore_index=True)\n",
        "# save the dataframe\n",
        "results_df.to_csv(\"results.csv\", index=False)\n",
        "results_df"
      ],
      "metadata": {
        "id": "i5QJaF1MHd7f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "results_df = pd.read_csv(\"results.csv\")\n",
        "results_df"
      ],
      "metadata": {
        "id": "bscmbzSexn0l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import keras\n",
        "import tensorflow as tf\n",
        "\n",
        "base_model = keras.models.load_model(\"models/embeddings_cbow_gru.keras\")\n",
        "\n",
        "raw_text_data = tf.constant([\n",
        "    [\"That was an awful movie, I hate it.\"],\n",
        "    [\"Not worth seeing.\"],\n",
        "    [\"That was an excellent movie, I love it. Best movie ever.\"],\n",
        "    [\"I was shocked. Such an unexpected ending! Can't wait to see it again\"],\n",
        "    [\"I was shocked. The movie was too short. Can't wait to see it again\"],\n",
        "], dtype=tf.string)\n",
        "\n",
        "# Vectorize the raw text data using the adapted TextVectorization layer\n",
        "processed_raw_text_data = text_vectorization(raw_text_data)\n",
        "\n",
        "# Make predictions using the base model with the vectorized numerical input\n",
        "predictions = base_model(processed_raw_text_data)\n",
        "\n",
        "for i in range(len(raw_text_data)):\n",
        "    # Access the scalar value by indexing predictions[i] with [0]\n",
        "    print(f\"text {i}: {float(predictions[i][0] * 100):.2f} percent positive\")"
      ],
      "metadata": {
        "id": "VB-Y5J-GIP7-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9krJ3kr_ipFK"
      },
      "source": [
        "## Simple Transformer model trained from scratch on word embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aIl5e-h7go17"
      },
      "source": [
        "### Transformer encoder implemented as a subclassed Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yqg_NyK8gN7o"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras import layers\n",
        "\n",
        "class TransformerEncoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.dense_dim = dense_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.attention = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.dense_proj = keras.Sequential(\n",
        "            [layers.Dense(dense_dim, activation=\"relu\"),\n",
        "             layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm_1 = layers.LayerNormalization()\n",
        "        self.layernorm_2 = layers.LayerNormalization()\n",
        "\n",
        "    def call(self, inputs, mask=None):\n",
        "        if mask is not None:\n",
        "            mask = mask[:, tf.newaxis, :]\n",
        "        attention_output = self.attention(\n",
        "            inputs, inputs, attention_mask=mask)\n",
        "        proj_input = self.layernorm_1(inputs + attention_output)\n",
        "        proj_output = self.dense_proj(proj_input)\n",
        "        return self.layernorm_2(proj_input + proj_output)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"embed_dim\": self.embed_dim,\n",
        "            \"num_heads\": self.num_heads,\n",
        "            \"dense_dim\": self.dense_dim,\n",
        "        })\n",
        "        return config"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-v0RuIiRHvoT"
      },
      "source": [
        "Define and train the Transformer model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6tHHfXKqhKKf"
      },
      "outputs": [],
      "source": [
        "vocab_size = 20000\n",
        "embed_dim = 256\n",
        "num_heads = 2\n",
        "dense_dim = 32\n",
        "\n",
        "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "x = layers.Embedding(vocab_size, embed_dim)(inputs)\n",
        "x = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\n",
        "x = layers.GlobalMaxPooling1D()(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.compile(optimizer=\"adam\", #\"rmsprop\",\n",
        "              loss=\"binary_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "model.summary()\n",
        "\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"transformer_encoder.keras\",\n",
        "                                    save_best_only=True)\n",
        "]\n",
        "model.fit(int_train_ds, validation_data=int_val_ds, epochs=20, callbacks=callbacks)\n",
        "model = keras.models.load_model(\n",
        "    \"transformer_encoder.keras\",\n",
        "    custom_objects={\"TransformerEncoder\": TransformerEncoder})\n",
        "print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_oUfKpiopq9K"
      },
      "source": [
        "## Simple Transformer model trained from scratch on positional embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26-zPuKwmYi0"
      },
      "source": [
        "Implement positional embedding as a subclassed layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LCiJ8ndxh50Q"
      },
      "outputs": [],
      "source": [
        "class CustomMaskingLayer(layers.Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return layers.Lambda(lambda x: tf.cast(tf.math.not_equal(x, 0), dtype=tf.bool))(inputs)\n",
        "\n",
        "\n",
        "    def compute_mask(self, inputs, mask=None):\n",
        "        return self.call(inputs)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        return config\n",
        "\n",
        "class PositionalEmbedding(layers.Layer):\n",
        "    def __init__(self, sequence_length, input_dim, output_dim, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.token_embeddings = layers.Embedding(\n",
        "            input_dim=input_dim, output_dim=output_dim, mask_zero=True) ## added\n",
        "        self.position_embeddings = layers.Embedding(\n",
        "            input_dim=sequence_length, output_dim=output_dim)\n",
        "        self.sequence_length = sequence_length\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "    def call(self, inputs):\n",
        "        length = tf.shape(inputs)[-1]\n",
        "        positions = tf.range(start=0, limit=length, delta=1)\n",
        "        embedded_tokens = self.token_embeddings(inputs)\n",
        "        embedded_positions = self.position_embeddings(positions)\n",
        "        return embedded_tokens + embedded_positions\n",
        "\n",
        "\n",
        "    def compute_mask(self, inputs, mask=None):\n",
        "        #return None\n",
        "        masking_layer = CustomMaskingLayer()\n",
        "        return masking_layer(inputs)\n",
        "        return layers.Lambda(lambda x: tf.cast(tf.math.not_equal(x, 0), dtype=tf.bool))(inputs)\n",
        "        return tf.not_equal(inputs, 0)\n",
        "        #return tf.cast(inputs != 0, dtype=tf.bool)\n",
        "        #return tf.math.not_equal(inputs, 0)\n",
        "        # Updated: Use TensorFlow compatible boolean mask creation\n",
        "        mask_layer = layers.Lambda(lambda x: tf.cast(x != 0, dtype=tf.bool))\n",
        "        return mask_layer(inputs)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"output_dim\": self.output_dim,\n",
        "            \"sequence_length\": self.sequence_length,\n",
        "            \"input_dim\": self.input_dim,\n",
        "        })\n",
        "        return config"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ldWZRsY-meq2"
      },
      "source": [
        "Combine the Transformer encoder with positional embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MdXbNrx7mOND"
      },
      "outputs": [],
      "source": [
        "class PositionalEmbedding(layers.Layer):\n",
        "    def __init__(self, sequence_length, input_dim, output_dim, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.token_embeddings = layers.Embedding(\n",
        "            input_dim=input_dim, output_dim=output_dim, mask_zero=True) ## added\n",
        "        self.position_embeddings = layers.Embedding(\n",
        "            input_dim=sequence_length, output_dim=output_dim)\n",
        "        self.sequence_length = sequence_length\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "    def call(self, inputs):\n",
        "        length = tf.shape(inputs)[-1]\n",
        "        positions = tf.range(start=0, limit=length, delta=1)\n",
        "        embedded_tokens = self.token_embeddings(inputs)\n",
        "        embedded_positions = self.position_embeddings(positions)\n",
        "        return embedded_tokens + embedded_positions\n",
        "\n",
        "    '''\n",
        "    def compute_mask(self, inputs, mask=None):\n",
        "        #return None\n",
        "        return tf.not_equal(inputs, 0)\n",
        "        #return tf.cast(inputs != 0, dtype=tf.bool)\n",
        "        #return tf.math.not_equal(inputs, 0)\n",
        "        # Updated: Use TensorFlow compatible boolean mask creation\n",
        "        mask_layer = layers.Lambda(lambda x: tf.cast(x != 0, dtype=tf.bool))\n",
        "        return mask_layer(inputs)\n",
        "    '''\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"output_dim\": self.output_dim,\n",
        "            \"sequence_length\": self.sequence_length,\n",
        "            \"input_dim\": self.input_dim,\n",
        "        })\n",
        "        return config"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 20000\n",
        "sequence_length = 600\n",
        "embed_dim = 256\n",
        "num_heads = 2\n",
        "dense_dim = 32\n",
        "\n",
        "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(inputs)\n",
        "x = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\n",
        "x = layers.GlobalAveragePooling1D()(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.compile(optimizer=\"adam\",    #\"rmsprop\",\n",
        "              loss=\"binary_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "model.summary()\n",
        "\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"full_transformer_encoder.keras\",\n",
        "                                    save_best_only=True)\n",
        "]\n",
        "model.fit(int_train_ds, validation_data=int_val_ds, epochs=10, callbacks=callbacks)\n",
        "model = keras.models.load_model(\n",
        "    \"full_transformer_encoder.keras\",\n",
        "    custom_objects={\"TransformerEncoder\": TransformerEncoder,\n",
        "                    \"PositionalEmbedding\": PositionalEmbedding})\n",
        "print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")"
      ],
      "metadata": {
        "id": "RhTkjhG-zAqd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Finetuning a Pretrained Transformer"
      ],
      "metadata": {
        "id": "uXrhsVcU0bj6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Loading a pretrained Transformer\n",
        "\n",
        "import keras_hub\n",
        "\n",
        "tokenizer = keras_hub.models.Tokenizer.from_preset(\"roberta_base_en\")\n",
        "backbone = keras_hub.models.Backbone.from_preset(\"roberta_base_en\")\n",
        "\n",
        "print(tokenizer(\"The quick brown fox\"))\n",
        "backbone.summary(line_length=80)"
      ],
      "metadata": {
        "id": "O_J-YqA00TER"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and process the data\n",
        "from keras.utils import text_dataset_from_directory\n",
        "\n",
        "batch_size = 16\n",
        "train_dir = \"aclImdb/train\"\n",
        "val_dir = \"aclImdb/val\"\n",
        "test_dir = \"aclImdb/test\"\n",
        "\n",
        "train_ds = text_dataset_from_directory(train_dir, batch_size=batch_size)\n",
        "val_ds = text_dataset_from_directory(val_dir, batch_size=batch_size)\n",
        "test_ds = text_dataset_from_directory(test_dir, batch_size=batch_size)\n",
        "\n",
        "def preprocess(text, label):\n",
        "    packer = keras_hub.layers.StartEndPacker(\n",
        "        sequence_length=512,\n",
        "        start_value=tokenizer.start_token_id,\n",
        "        end_value=tokenizer.end_token_id,\n",
        "        pad_value=tokenizer.pad_token_id,\n",
        "        return_padding_mask=True,\n",
        "    )\n",
        "    token_ids, padding_mask = packer(tokenizer(text))\n",
        "    return {\"token_ids\": token_ids, \"padding_mask\": padding_mask}, label\n",
        "\n",
        "preprocessed_train_ds = train_ds.map(preprocess)\n",
        "preprocessed_val_ds = val_ds.map(preprocess)\n",
        "preprocessed_test_ds = test_ds.map(preprocess)\n",
        "\n",
        "next(iter(preprocessed_train_ds))"
      ],
      "metadata": {
        "id": "TVQIhKzO1Bwc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create and finetune the model\n",
        "\n",
        "inputs = backbone.input\n",
        "x = backbone(inputs)\n",
        "x = x[:, 0, :]\n",
        "x = layers.Dropout(0.1)(x)\n",
        "x = layers.Dense(768, activation=\"relu\")(x)\n",
        "x = layers.Dropout(0.1)(x)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "classifier = keras.Model(inputs, outputs)\n",
        "\n",
        "\n",
        "\n",
        "classifier.compile(\n",
        "    optimizer=keras.optimizers.Adam(5e-5),\n",
        "    loss=\"binary_crossentropy\",\n",
        "    metrics=[\"accuracy\"],\n",
        ")\n",
        "classifier.fit(\n",
        "    preprocessed_train_ds,\n",
        "    validation_data=preprocessed_val_ds,\n",
        "    epochs = 1\n",
        ")\n",
        "\n",
        "classifier.evaluate(preprocessed_test_ds)"
      ],
      "metadata": {
        "id": "nOtu8MOI1Ixd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## To be done\n",
        "- bug: model name in dataframe\n",
        "- change optimizer to Adam\n",
        "- rerun for CBOW\n",
        "- new code for Transformer\n",
        "- finish and run the pretrained transformer part"
      ],
      "metadata": {
        "id": "oUrG_oG8yYYc"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}